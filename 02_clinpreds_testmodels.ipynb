{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#only need to run these if packages haven't been installed yet\n",
    "#!{sys.executable} -m pip install numpy\n",
    "#!{sys.executable} -m pip install pandas\n",
    "#!{sys.executable} -m pip install sklearn\n",
    "#!{sys.executable} -m pip install matplotlib\n",
    "#!{sys.executable} -m pip install datetime\n",
    "#!{sys.executable} -m pip install seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, GroupKFold, GroupShuffleSplit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean up ABCD data\n",
    "# set base dirctories\n",
    "ABCD_base_dir   = 'base_directory_path'\n",
    "\n",
    "#load subj fc data\n",
    "ABCD_fc_df = pd.read_csv(os.path.join(ABCD_base_dir, 'fc_data.csv'), header=None)\n",
    "ABCD_fc_subj = pd.read_csv(os.path.join(ABCD_base_dir, 'fc_subj_data.txt'), header=None)\n",
    "ABCD_fc = ABCD_fc_df.T\n",
    "\n",
    "# load subj demo and clinical data\n",
    "ABCD_subj = pd.read_csv(os.path.join(ABCD_base_dir, 'clin_subj_data.csv'))\n",
    "ABCD_clin = pd.read_csv(os.path.join(ABCD_base_dir, 'clin_subj_data.csv'))\n",
    "\n",
    "#drop duplicate header rows\n",
    "header_row = 0\n",
    "ABCD_subj = ABCD_subj.drop(header_row)\n",
    "ABCD_clin = ABCD_clin.drop(header_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add subj key data to fc data and sort\n",
    "ABCD_fc_subj.columns=['subjectkey']\n",
    "ABCD_fc.insert(0, \"subjectkey\", ABCD_fc_subj, True)\n",
    "ABCD_fc_sorted = ABCD_fc.sort_values(by='subjectkey', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and sort clinical data\n",
    "mask = ABCD_clin.subjectkey.isin(ABCD_fc_sorted['subjectkey'])\n",
    "ABCD_clin_subjs = ABCD_clin[mask]\n",
    "ABCD_clin_baseline = ABCD_clin_subjs[ABCD_clin_subjs.eventname == 'baseline_year_1_arm_1']\n",
    "ABCD_clin_sorted = ABCD_clin_baseline.sort_values(by='subjectkey', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and sort clinical data\n",
    "mask = ABCD_subj.subjectkey.isin(ABCD_fc_sorted['subjectkey'])\n",
    "ABCD_subj_incl = ABCD_subj[mask]\n",
    "ABCD_subj_baseline = ABCD_subj_incl[ABCD_subj_incl.eventname == 'baseline_year_1_arm_1']\n",
    "ABCD_subj_sorted = ABCD_subj_baseline.sort_values(by='subjectkey', ascending=True)\n",
    "ABCD_subj_data = ABCD_subj_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate and clean clinical variable to be predicted\n",
    "ABCD_clin_data = ABCD_clin_sorted[['cbcl_scr_syn_anxdep_r', 'cbcl_scr_syn_withdep_r',\n",
    "                                  'cbcl_scr_syn_somatic_r', 'cbcl_scr_syn_social_r',\n",
    "                                  'cbcl_scr_syn_thought_r', 'cbcl_scr_syn_attention_r',\n",
    "                                  'cbcl_scr_syn_rulebreak_r', 'cbcl_scr_syn_aggressive_r',\n",
    "                                  'cbcl_scr_syn_internal_r',  'cbcl_scr_syn_external_r',\n",
    "                                  'cbcl_scr_syn_totprob_r', 'cbcl_scr_dsm5_depress_r',\n",
    "                                  'cbcl_scr_dsm5_anxdisord_r', 'cbcl_scr_dsm5_somaticpr_r',\n",
    "                                  'cbcl_scr_dsm5_adhd_r', 'cbcl_scr_dsm5_opposit_r',\n",
    "                                  'cbcl_scr_dsm5_conduct_r', 'cbcl_scr_07_sct_r',\n",
    "                                  'cbcl_scr_07_ocd_r', 'cbcl_scr_07_stress_r']]\n",
    "\n",
    "ABCD_clin_labels = ['AnxDep', 'WithDep', 'Somatic', 'Social', 'Thought', 'Attention',\n",
    "                    'RuleBreak', 'Aggresive', 'Internal', 'External', 'TotProb', 'Depress',\n",
    "                    'AnxDiscord', 'SomaticPr', 'ADHD', 'Opposit', 'Conduct', 'Sluggish', \n",
    "                    'OCD', 'Stress']\n",
    "\n",
    "ABCD_clin_data.columns = ABCD_clin_labels\n",
    "ABCD_clin_data.reset_index(inplace=True)\n",
    "ABCD_clin_data = ABCD_clin_data.drop(columns=['index'])\n",
    "\n",
    "#clean fc data \n",
    "ABCD_fc_data = ABCD_fc_sorted.drop(columns=['subjectkey'])\n",
    "ABCD_fc_data.reset_index(inplace=True) \n",
    "ABCD_fc_data = ABCD_fc_data.drop(columns=['index'])\n",
    "\n",
    "#clean subj data\n",
    "ABCD_subj_data.reset_index(inplace=True) \n",
    "ABCD_subj_data = ABCD_subj_data.drop(columns=['index'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sex-specific variables\n",
    "mask_m = ABCD_subj_sorted.sex=='M'\n",
    "ABCD_subj_m = ABCD_subj_sorted[mask_m]\n",
    "ABCD_clin_m = ABCD_clin_data[mask_m]\n",
    "ABCD_fc_m = ABCD_fc_data[mask_m]\n",
    "\n",
    "mask_f = ABCD_subj_sorted.sex=='F'\n",
    "ABCD_subj_f = ABCD_subj_sorted[mask_f]\n",
    "ABCD_clin_f = ABCD_clin_data[mask_f]\n",
    "ABCD_fc_f = ABCD_fc_data[mask_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of repetitions you want to perform\n",
    "rep = 100\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .66\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "regr = Ridge(normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "#set x data to be the input variable you want to use\n",
    "X_m = ABCD_fc_m\n",
    "X_f = ABCD_fc_f\n",
    "\n",
    "Y_m = ABCD_clin_m\n",
    "Y_f = ABCD_clin_f\n",
    "\n",
    "#number of variables \n",
    "#iterating through all of the clinical variables\n",
    "n_beh = Y_f.shape[1]\n",
    "\n",
    "#number of features \n",
    "n_feat = X_f.shape[1]\n",
    "\n",
    "#test within sex only here\n",
    "n_test = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create arrays to store variables\n",
    "\n",
    "#r^2 - coefficient of determination\n",
    "r2_mm = np.zeros([rep,n_beh])\n",
    "r2_mf = np.zeros([rep,n_beh])\n",
    "r2_fm = np.zeros([rep,n_beh])\n",
    "r2_ff = np.zeros([rep,n_beh])\n",
    "\n",
    "#explained variance\n",
    "var_mm = np.zeros([rep,n_beh])\n",
    "var_mf = np.zeros([rep,n_beh])\n",
    "var_fm = np.zeros([rep,n_beh])\n",
    "var_ff = np.zeros([rep,n_beh])\n",
    "\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr_mm = np.zeros([rep,n_beh])\n",
    "corr_mf = np.zeros([rep,n_beh])\n",
    "corr_fm = np.zeros([rep,n_beh])\n",
    "corr_ff = np.zeros([rep,n_beh])\n",
    "\n",
    "\n",
    "#feature importance extracted from the model\n",
    "featimp_m = np.zeros([rep,n_feat])\n",
    "featimp_f = np.zeros([rep,n_feat])\n",
    "#for when the feat weights get haufe-inverted\n",
    "#featimp_haufe_m = np.zeros([rep,n_feat])\n",
    "#featimp_haufe_f = np.zeros([rep,n_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir   = 'results_directory_path'\n",
    "\n",
    "#iterate through train behaviors 1 at a time\n",
    "beh_train = 0\n",
    "\n",
    "#ABCD_clin_labels = ['AnxDep', 'WithDep', 'Somatic', 'Social', 'Thought', 'Attention',\n",
    "#                    'RuleBreak', 'Aggresive', 'Internal', 'External', 'TotProb', 'Depress',\n",
    "#                    'AnxDiscord', 'SomaticPr', 'ADHD', 'Opposit', 'Conduct', 'Sluggish', \n",
    "#                    'OCD', 'Stress']\n",
    "\n",
    "clin_var = ABCD_clin_labels[beh_train]\n",
    "print(clin_var)\n",
    "\n",
    "#load in optimized alpha from when models were trained\n",
    "opt_alpha_m = np.load(results_dir + '/fc_alpha_m_' + clin_var + '.npy')\n",
    "opt_alpha_f = np.load(results_dir + '/fc_alpha_f_' + clin_var + '.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#iterate through number of models\n",
    "for p in range(rep):\n",
    "    #print model # you're on\n",
    "    print('Model %d' %(p+1))\n",
    "    \n",
    "    #print time\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    \n",
    "    #split data into train/test\n",
    "    train_inds_m, test_inds_m = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_m, groups=ABCD_subj_m['site_id_l']))\n",
    "    train_inds_f, test_inds_f = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X_f, groups=ABCD_subj_f['site_id_l']))\n",
    "    #x_train, x_test, cog_train, cog_test = train_test_split(X, Y, test_size=1-train_size, shuffle=True, random_state=p)\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train_m = X_m.iloc[train_inds_m].values\n",
    "    x_test_m = X_m.iloc[test_inds_m].values\n",
    "        \n",
    "    #set y values based on indices from split  \n",
    "    beh_train_m = Y_m.iloc[train_inds_m].values\n",
    "    beh_test_m = Y_m.iloc[test_inds_m].values \n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train_f = X_f.iloc[train_inds_f].values\n",
    "    x_test_f = X_f.iloc[test_inds_f].values\n",
    "        \n",
    "    #set y values based on indices from split  \n",
    "    beh_train_f = Y_f.iloc[train_inds_f].values\n",
    "    beh_test_f = Y_f.iloc[test_inds_f].values \n",
    "    \n",
    "    #convert y values to to double\n",
    "    y_train_m = np.double(beh_train_m)[:, beh_train]\n",
    "    y_train_f = np.double(beh_train_f)[:, beh_train]\n",
    "        \n",
    "\n",
    "    #fit model to train data\n",
    "    model_m = Ridge(alpha = opt_alpha_m[p], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "    model_m.fit(x_train_m, y_train_m);\n",
    "        \n",
    "    model_f = Ridge(alpha = opt_alpha_f[p], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "    model_f.fit(x_train_f, y_train_f);\n",
    "        \n",
    "        \n",
    "    #iterate through all of the clinical variables to see if models generalize across sexes and across behaviors\n",
    "    for beh_test in range(n_beh):\n",
    "        print (\"Testing Behaviour: %s\" % Y_m.columns[beh_test])\n",
    "            \n",
    "        y_test_m = np.double(beh_test_m)[:,beh_test]\n",
    "        y_test_f = np.double(beh_test_f)[:,beh_test]\n",
    "    \n",
    "        r2_mm[p,beh_test]=model_m.score(x_test_m,y_test_m)\n",
    "        r2_mf[p,beh_test]=model_m.score(x_test_f,y_test_f)\n",
    "        \n",
    "        r2_fm[p,beh_test]=model_f.score(x_test_m,y_test_m)\n",
    "        r2_ff[p,beh_test]=model_f.score(x_test_f,y_test_f)\n",
    "        \n",
    "        #generate predictions within and between sexes\n",
    "        preds_mm = []\n",
    "        preds_mf = []\n",
    "        preds_fm = []\n",
    "        preds_ff = []\n",
    "        \n",
    "        preds_mm = model_m.predict(x_test_m).ravel()\n",
    "        preds_mf = model_m.predict(x_test_f).ravel()\n",
    "        preds_fm = model_f.predict(x_test_m).ravel()\n",
    "        preds_ff = model_f.predict(x_test_f).ravel()\n",
    "        \n",
    "        \n",
    "        #compute explained variance \n",
    "        var_mm[p,beh_test] = explained_variance_score(y_test_m, preds_mm)\n",
    "        var_mf[p,beh_test] = explained_variance_score(y_test_f, preds_mf)\n",
    "        \n",
    "        var_fm[p,beh_test] = explained_variance_score(y_test_m, preds_fm)\n",
    "        var_ff[p,beh_test] = explained_variance_score(y_test_f, preds_ff)\n",
    "\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr_mm[p,beh_test] = np.corrcoef(y_test_m.ravel(), preds_mm)[1,0]\n",
    "        corr_mf[p,beh_test] = np.corrcoef(y_test_f.ravel(), preds_mf)[1,0]\n",
    "        \n",
    "        corr_fm[p,beh_test] = np.corrcoef(y_test_m.ravel(), preds_fm)[1,0]\n",
    "        corr_ff[p,beh_test] = np.corrcoef(y_test_f.ravel(), preds_ff)[1,0]\n",
    "        \n",
    "        \n",
    "        #print (\"Haufe-Transforming Feature Weights\")\n",
    "        #cov_x = []\n",
    "        #cov_y = []\n",
    "    \n",
    "        #extract feature importance\n",
    "        featimp_m[p,:] = model_m.coef_\n",
    "        featimp_f[p,:] = model_f.coef_\n",
    "        #compute Haufe-inverted feature weights\n",
    "        #cov_x_m = np.cov(np.transpose(x_train_m))\n",
    "        #cov_y_m = np.cov(y_train_m)\n",
    "        #featimp_haufe_m[p,:] = np.matmul(cov_x_m,featimp_m[p,:])*(1/cov_y_m)\n",
    "        \n",
    "        #cov_x_f = np.cov(np.transpose(x_train_f))\n",
    "        #cov_y_f = np.cov(y_train_f)\n",
    "        #featimp_haufe_f[p,:] = np.matmul(cov_x_f,featimp_f[p,:])*(1/cov_y_f)\n",
    "\n",
    "\n",
    "    # save results\n",
    "    results_dir   = 'results_directory_path'\n",
    "\n",
    "    np.save((results_dir + '/fc_featimp_m_' + clin_var + '.npy'),featimp_m)\n",
    "    np.save((results_dir + '/fc_featimp_f_' + clin_var + '.npy'),featimp_f)    \n",
    "    \n",
    "    np.save((results_dir + '/fc_crossbehav_r2_mm_' + clin_var + '.npy'),r2_mm)\n",
    "    np.save((results_dir + '/fc_crossbehav_var_mm_' + clin_var + '.npy'),var_mm)\n",
    "    np.save((results_dir + '/fc_crossbehav_corr_mm_' + clin_var + '.npy'),corr_mm)\n",
    "    \n",
    "    np.save((results_dir + '/fc_crossbehav_r2_mf_' + clin_var + '.npy'),r2_mf)\n",
    "    np.save((results_dir + '/fc_crossbehav_var_mf_' + clin_var + '.npy'),var_mf)\n",
    "    np.save((results_dir + '/fc_crossbehav_corr_mf_' + clin_var + '.npy'),corr_mf)\n",
    "    \n",
    "    np.save((results_dir + '/fc_crossbehav_r2_fm_' + clin_var + '.npy'),r2_fm)\n",
    "    np.save((results_dir + '/fc_crossbehav_var_fm_' + clin_var + '.npy'),var_fm)\n",
    "    np.save((results_dir + '/fc_crossbehav_corr_fm_' + clin_var + '.npy'),corr_fm)\n",
    "\n",
    "    np.save((results_dir + '/fc_crossbehav_r2_ff_' + clin_var + '.npy'),r2_ff)\n",
    "    np.save((results_dir + '/fc_crossbehav_var_ff_' + clin_var + '.npy'),var_ff)\n",
    "    np.save((results_dir + '/fc_crossbehav_corr_ff_' + clin_var + '.npy'),corr_ff)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
